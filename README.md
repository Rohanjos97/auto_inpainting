# Conferece:
## Sharma, S., Joshi, R., Bhilare, S. et al. Robust Adversarial Defense: An Analysis on Use of Auto-Inpainting. SN COMPUT. SCI. 6, 17 (2025). https://doi.org/10.1007/s42979-024-03542-5

### Abstract
In recent years, adversarial patch attacks have become a major concern since they can seriously compromise the security and reliability of deep neural networks. These attacks involve modifying a clean image by adding a patch with carefully crafted pixels, which can confuse the network and cause it to misclassify the image. Our novel approach addresses this problem by removing the patch from the image and restoring the missing pixels using various image inpainting techniques. We automatically detect the adversarial patch using a method called Fast Score Class Activation Map which localizes the salient region in an image. Further, we replace the patch with inpainted pixels that utilize the neighbourhing pixels and a local or a global context in the image. This approach ensures the integrity of the image’s original structure while effectively tackling the adversarial nature of the patch, regardless of whether it is universal, robust, or targeted. Importantly, our method works even in the black-box setting without assuming specific details about the patch such as its location, shape or size, making it suitable for deployment. Moreover, our approach does not necessitate retraining the neural network on adversarial examples, thus, adding to its practicality in real-world scenarios. Our experiments show that adversarial patch attacks can reduce classification accuracy on the ImageNet100 dataset from 96.78% to as low as 0.01%. The proposed black-box method recovers classification accuracy to 80.89%, while the best-performing white-box method reaches only 76.80%. Furthermore, the defense is oblivious to the input being adversarial or benign and achieves an accuracy of 84.20% on clean images demonstrating its effectiveness in either case. This defense is especially relevant for real-world applications, such as autonomous vehicles and surveillance systems, where adversarial patches can compromise critical decisions by misleading object detection and classification.

DOI: https://doi.org/10.1007/s42979-024-03542-5

<br><br>
# Journal:
## Sharma, S., Joshi, R., Bhilare, S., Joshi, M.V. (2023). Robust Adversarial Defence: Use of Auto-inpainting. In: Tsapatsoulis, N., et al. Computer Analysis of Images and Patterns. CAIP 2023. Lecture Notes in Computer Science, vol 14184. Springer, Cham. https://doi.org/10.1007/978-3-031-44237-7_11

# Abstract
Adversarial patch attacks have become a primary concern in recent years as they pose a significant threat to the security and reliability of deep neural networks. Modifying benign images by introducing adversarial patches comprising localized adversarial pixels alters the salient features of the image resulting in misclassification. The novelty of our approach is in the use of image inpainting technique as an adversarial defence for rectifying the patch region. Adversarial patch is automatically localized using Fast Score Class Activation Map and superseded by inpainting using Fast Marching Method which efficiently propagates pixel information from the surrounding areas into the patch region. This approach ensures original image’s structural integrity while simultaneously inpainting the adversarial pixels. Moreover, at the time of the attack it is not expected to have prior knowledge about the patch. Therefore, we propose our novel adversarial defence technique in a black-box setting assuming no knowledge about the patch location, shape or its size. Furthermore, we do not rely on re-training our victim model on adversarial examples, indicating its potential usefulness for real-world applications. Our experimental results show that the proposed approach achieves accuracy up to 76.37% on ImageNet100 despite the adversarial patch attack amounting to a considerable improvement of 76.28% points. Moreover, on benign images our approach gives decent accuracy of  thereby suggesting that our defence pipeline is applicable irrespective of whether the input image is adversarial or clean.

DOI: https://doi.org/10.1007/978-3-031-44237-7_11
